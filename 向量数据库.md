# 向量数据库

向量数据库（Vector Database）是一种专门用于存储、索引和管理高维向量数据（Vector Data）的数据库类型。它的设计目标是高效地处理与向量之间的相似性搜索（Similarity Search）或最近邻搜索（Nearest Neighbor Search），可以根据相似性指标而不是精确匹配来识别数据，从而使计算机模型能够理解数据的上下文。广泛应用于人工智能（AI）、机器学习（ML）、推荐系统、自然语言处理（NLP）、计算机视觉等领域。

## 概述

向量数据是由一组数值组成的多维数组，通常用来表示对象的特征或嵌入（Embedding）。这些向量可以通过机器学习模型生成，例如：

- 文本嵌入：通过词嵌入模型（如 Word2Vec、BERT）将单词或句子转化为高维向量（例如 300 维或 768 维），表示其语义信息。
- 图像嵌入：通过卷积神经网络（CNN）将图像转化为向量，表示其视觉特征。
- 音频嵌入：通过音频处理模型将声音片段转化为向量。

例如，一个 3 维向量可能是 [0.1, -0.5, 2.3]，而一个 768 维向量可能是从 BERT 模型输出的文本表示。
这些向量的核心特性是它们在高维空间中的位置可以通过数学计算（如欧几里得距离、余弦相似度）来衡量相似性。

## 核心功能

与传统关系型数据库（RDBMS，如 MySQL）或键值数据库（Key-Value Store，如 Redis）不同，向量数据库专注于以下功能：

- **向量存储**：高效存储高维向量数据，通常以二进制或浮点数形式。
- **相似性搜索**：快速找到与给定查询向量最相似的向量集合。
  - 常用度量方法：
    1. 欧几里得距离（Euclidean Distance）：衡量直线距离。
    2. 余弦相似度（Cosine Similarity）：衡量角度相似性，常用在文本或语义搜索中。
    3. 曼哈顿距离（Manhattan Distance） 或其他距离函数。
- **索引优化**：使用专门的索引结构（如 HNSW、IVF、PQ）加速搜索，减少计算复杂度。
- **实时更新**：支持向量的插入、删除和更新，适用于动态数据。
- **可扩展性**：支持大规模数据集（百万到十亿级别向量）的高效查询。

## 工作原理

1. 数据嵌入：
使用预训练模型（如 BERT、CLIP）将原始数据（文本、图像等）转化为向量。
例如，输入“猫”可能生成向量 [0.12, -0.34, 0.89, ...]。
2. 向量存储：
将生成的向量存储在数据库中，通常伴随元数据（例如 ID、标签）。
3. 索引构建：
   - 创建索引（如 Hierarchical Navigable Small World, HNSW，或 Inverted File with Product Quantization, IVF-PQ），将向量组织成便于搜索的结构。
   - 索引通过近似最近邻（Approximate Nearest Neighbor, ANN）算法（如 Annoy、Faiss）提高查询效率。
4. 查询处理：
输入一个查询向量，数据库通过索引找到 k 个最近邻向量。
返回结果可能包括向量本身或关联的元数据。

示例：查询“猫”的向量，数据库可能返回与“猫”语义相近的向量（如“ kitten”、“ feline”）对应的文本或图像。

## 与传统数据库对比

| 特性 | 传统数据库 (如 MySQL) | 向量数据库 |
| ----------- | ----------- |  ----------- |
| 数据类型 | 结构化数据（表格、键值对） | 高维向量数据 |
| 查询类型 | 精确匹配（WHERE、JOIN） | 相似性搜索（最近邻） |
| 索引 | B-Tree、Hash 索引 | ANN 索引（如 HNSW、IVF） |
| 用例 | 财务数据、用户记录 | AI 应用、推荐系统、搜索 |
| 性能优化 | 事务一致性、查询速度 | 近似搜索效率、向量距离计算 |

## 如何建立关联性

假设需要将cat 和 hello kitty 建立关联性

### 方法 1：调整生成向量数据（嵌入阶段）

- **使用相同的嵌入表示**：
使用预训练模型（如 Sentence-BERT）生成初始向量：
cat → [0.1, -0.2, 0.3, ...]（假设 768 维）。
Hello Kitty → [0.4, 0.1, -0.5, ...]（初始向量）。
手动将“Hello Kitty”的向量替换为“cat”的向量，或者取两者的平均值：
Hello Kitty → [0.1, -0.2, 0.3, ...]（与“cat”相同）。
将调整后的向量存储到向量数据库。
  - 优点：简单直接，强制定义相似性。
  - 缺点：可能破坏模型的语义一致性，影响其他查询结果。
- **微调嵌入模型**：
如果你有大量数据（例如“cat”和“Hello Kitty”相关的语料），可以微调预训练模型，使其在生成嵌入时自然地将“cat”和“Hello Kitty”拉近。
实现：
准备数据集，例如包含“cat”和“Hello Kitty”共同出现的句子（如“cat is like Hello Kitty”）。
使用监督学习或对比学习（如 triplet loss）微调模型，使“cat”和“Hello Kitty”的向量距离变小。
生成新向量并存储。
  - 优点：语义上更自然，适用于大规模自定义。
  - 缺点：需要数据和计算资源，复杂性较高。

### 方法 2：后期处理（数据库层面的关系定义）

在向量生成后，通过数据库的元数据或后处理逻辑定义关系，而不改变向量本身：

- **添加元数据标签**：
为“cat”和“Hello Kitty”添加相同的自定义标签（如 category: feline_character）。
实现：
存储向量时，附带元数据：
cat → {vector: [0.1, -0.2, 0.3, ...], metadata: {category: "feline_character"}}
Hello Kitty → {vector: [0.4, 0.1, -0.5, ...], metadata: {category: "feline_character"}}
在查询时，结合向量相似性搜索和元数据过滤，优先返回带有相同标签的向量。
  - 优点：灵活，保留原始向量语义，同时支持自定义关系。
  - 缺点：需要额外的查询逻辑，可能增加复杂性。
- **定义邻居关系**：
一些向量数据库（如 Milvus、Pinecone）支持手动指定“邻居”或“关联”关系。你可以告诉数据库“cat”和“Hello Kitty”是近邻。
实现：
使用数据库的 API 手动添加关联（例如 Milvus 的 add_neighbors 或 Pinecone 的自定义索引）。
查询时，数据库会优先考虑这些关系。
  - 优点：直接控制关系，适合少量自定义。
  - 缺点：依赖数据库支持，扩展性有限。

### 方法 3：查询时的后处理

```python
import faiss
import numpy as np

# 假设已有向量数据
d = 64  # 向量维度
xb = np.random.random((1000, d)).astype('float32')  # 1000 个向量
index = faiss.IndexFlatL2(d)
index.add(xb)

# 查询向量
xq = np.random.random((1, d)).astype('float32')  # 假设是 "cat" 的向量
k = 5
distances, indices = index.search(xq, k)

# 手动添加 "Hello Kitty" 的索引（假设索引为 999）
hello_kitty_idx = 999
if hello_kitty_idx not in indices[0]:
    indices[0] = np.append(indices[0], hello_kitty_idx)[:k]
    distances[0] = np.append(distances[0], 0.0)[:k]  # 强制相似度为 0

print("Adjusted indices:", indices)
```

- 优点：灵活，适用于动态调整。
- 缺点：需要在每次查询时处理，性能可能受影响。

## 参考链接

- [什么是向量数据库](https://www.cloudflare.com/zh-cn/learning/ai/what-is-vector-database/)